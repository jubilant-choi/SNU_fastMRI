{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba08abfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from utils.data.load_data import create_data_loaders\n",
    "from utils.common.utils import save_reconstructions, ssim_loss\n",
    "from utils.common.loss_function import SSIMLoss\n",
    "# from utils.model.unet import Unet\n",
    "# from utils.model.unet import Unet2\n",
    "\n",
    "# unet = Unet(in_chans = 1, out_chans = 1)\n",
    "# unet2 = Unet2(in_chans = 1, out_chans = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b82022c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Code\t\t\t\t   logs\t\t       run_Unet_JB.py   utils\n",
      " brain_leaderboard_state_dict.pt  'model test.ipynb'   run_Unet_SJ.py\n",
      " evaluate.py\t\t\t   plot.py\t       test_run.py\n",
      " leaderboard_eval.py\t\t   result\t       train.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83beb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv policy.py utils/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ec8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c3ad114",
   "metadata": {},
   "source": [
    "# Testing model's input & output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43ff0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.Tensor(1, 384, 384)\n",
    "print(dummy_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe9ca59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 384]) torch.Size([1, 384, 384])\n"
     ]
    }
   ],
   "source": [
    "output1 = unet(dummy_input)\n",
    "output2 = unet2(dummy_input)\n",
    "print(output1.shape, output2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca64e3",
   "metadata": {},
   "source": [
    "# Testing model load & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c86f7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/SJ/test_Unet/checkpoints\n",
      "['best_model.pt', 'model.pt']\n",
      "\n",
      "./result/JB/Unet/checkpoints\n",
      "[]\n",
      "\n",
      "./result/JB/newUnet/checkpoints\n",
      "['best_model.pt', 'model.pt', 'newUnet_test02_epoch30.pt', 'newUnet_test02_best.pt']\n",
      "\n",
      "./result/JB/test_Unet/checkpoints\n",
      "['best_model.pt', 'model.pt']\n",
      "\n",
      "./result/test_Unet/checkpoints\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk('./result'):\n",
    "    if root.endswith('/checkpoints'):\n",
    "        print(root)\n",
    "        print(files)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d319e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f='/root/SNU_fastMRI/result/JB/newUnet/checkpoints/newUnet_test02_epoch30.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52b56768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model.unet_advanced import Unet as newUnet\n",
    "net = newUnet(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5c911d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4bd1af0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0005,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False,\n",
       " 'params': [0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['optimizer']['param_groups'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "58192de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(c['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24547c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (down_sample_layers): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv): ConvBlock(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (3): Dropout2d(p=0.0, inplace=False)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (5): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (7): Dropout2d(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (up_conv): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Dropout2d(p=0.0, inplace=False)\n",
       "        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (5): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (7): Dropout2d(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvBlock(\n",
       "        (layers): Sequential(\n",
       "          (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (3): Dropout2d(p=0.0, inplace=False)\n",
       "          (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "          (7): Dropout2d(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (up_transpose_conv): ModuleList(\n",
       "    (0): TransposeConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransposeConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): TransposeConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (3): TransposeConvBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)\n",
       "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4057d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/hi\n"
     ]
    }
   ],
   "source": [
    "a = Path('/root')\n",
    "b = 'hi'\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9436db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists('result/JB/newUnet/checkpoints/newUnet_test02_best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7417c0",
   "metadata": {},
   "source": [
    "# testing json load and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a132bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result/JB/newUnet/jsons/newUnet_test02.json','r') as f:\n",
    "    res = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a514ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlo = res['train_losses'][:31]\n",
    "vlo = res['val_losses'][:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab71e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tln = res['train_losses'][-60:]\n",
    "vln = res['val_losses'][-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd9fea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['train_losses'] = tlo + tln\n",
    "res['val_losses'] = vlo + vln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db6453c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/JB/newUnet/jsons/newUnet_test02.json', 'w') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e0ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "if os.getcwd() + '/utils/model/' not in sys.path:\n",
    "    sys.path.insert(1, os.getcwd() + '/utils/model/')\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "from utils.learning.train_part import train\n",
    "from utils.common.utils import save_exp_result\n",
    "from pathlib import Path\n",
    "\n",
    "def parse(c):\n",
    "    parser = argparse.ArgumentParser(description='Train Unet on FastMRI challenge Images',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('-g', '--GPU-NUM', type=int, default=0, help='GPU number to allocate')\n",
    "    parser.add_argument('-b', '--batch-size', type=int, default=256, help='Batch size')\n",
    "    parser.add_argument('-e', '--num-epochs', type=int, default=3, help='Number of epochs')\n",
    "    parser.add_argument('-l', '--lr', type=float, default=1e-3, help='Learning rate')\n",
    "    parser.add_argument('-r', '--report-interval', type=int, default=10, help='Report interval')\n",
    "    parser.add_argument('-n', '--net-name', type=Path, required=True, help='Name of network')\n",
    "    parser.add_argument('-o', '--optim', type=str, default='Adam', help='Name of optimizer')\n",
    "    parser.add_argument('-s', '--scheduler', type=str, default='Plateau', help='Name of lr scheduler')\n",
    "    \n",
    "    parser.add_argument('-t', '--data-path-train', type=Path, default='/data/fastMRI/train/', help='Directory of train data')\n",
    "    parser.add_argument('-v', '--data-path-val', type=Path, default='/data/fastMRI/val/', help='Directory of validation data')\n",
    "    parser.add_argument('--cascade', type=int, default=1, help='Number of cascades | Should be less than 12')\n",
    "    \n",
    "    parser.add_argument('--in-chans', type=int, default=1, help='Size of input channels for network')\n",
    "    parser.add_argument('--out-chans', type=int, default=1, help='Size of output channels for network')\n",
    "    parser.add_argument('--input-key', type=str, default='image_input', help='Name of input key')\n",
    "    parser.add_argument('--target-key', type=str, default='image_label', help='Name of target key')\n",
    "    parser.add_argument('--max-key', type=str, default='max', help='Name of max key in attributes')\n",
    "    \n",
    "    parser.add_argument('--load', type=str, default='', help='Name of saved model that will be loaded')\n",
    "    parser.add_argument('-u', '--user', type=str, choices=['SJ','JB'], required=True, help='User name')\n",
    "    parser.add_argument('-x', '--exp-name', type=str, default='test', help='Name of an experiment')\n",
    "      \n",
    "    args = parser.parse_args(c)\n",
    "#     tot_iter = 5164\n",
    "#     args.report_interval = int((tot_iter/args.batch_size)/10)\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8fc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = '-u JB -x AdaptiveVarNet1 -n AdaptiveVarNet -b 1 -e 1 -l 1e-5 -s P --input-key kspace --cascade 1'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47734cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba60cb56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(GPU_NUM=0, batch_size=1, cascade=1, data_path_train=PosixPath('/data/fastMRI/train'), data_path_val=PosixPath('/data/fastMRI/val'), exp_name='AdaptiveVarNet1', in_chans=1, input_key='kspace', load='', lr=1e-05, max_key='max', net_name=PosixPath('AdaptiveVarNet'), num_epochs=1, optim='Adam', out_chans=1, report_interval=10, scheduler='P', target_key='image_label', user='JB')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8ecf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24339fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.model.adaptive_varnet import AdaptiveVarNet\n",
    "\n",
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "torch.cuda.empty_cache()\n",
    "model = AdaptiveVarNet(num_cascades=64).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa8ce62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading state_dict: 100%|███████████████████████████████████████████████████| 651M/651M [01:41<00:00, 6.40MiB/s]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_lightning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m url_root \u001b[38;5;241m=\u001b[39m AdaptiveVarNet_FOLDER\n\u001b[1;32m     30\u001b[0m download_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://dl.fbaipublicfiles.com/active-mri-acquisition/midl_models/adaptive_4x.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m, MODEL_FNAMES)\n\u001b[0;32m---> 32\u001b[0m pretrained \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_FNAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m pretrained_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(pretrained)\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:1039\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "if os.getcwd() + '/utils/model/' not in sys.path:\n",
    "    sys.path.insert(1, os.getcwd() + '/utils/model/')\n",
    "import requests    \n",
    "from tqdm import tqdm\n",
    "from utils.model.adaptive_varnet import AdaptiveVarNet\n",
    "\n",
    "def download_model(url, fname):\n",
    "    response = requests.get(url, timeout=10, stream=True)\n",
    "\n",
    "    chunk_size = 8 * 1024 * 1024  # 8 MB chunks\n",
    "    total_size_in_bytes = int(response.headers.get(\"content-length\", 0))\n",
    "    progress_bar = tqdm(\n",
    "        desc=\"Downloading state_dict\",\n",
    "        total=total_size_in_bytes,\n",
    "        unit=\"iB\",\n",
    "        unit_scale=True,\n",
    "    )\n",
    "\n",
    "    with open(fname, \"wb\") as fh:\n",
    "        for chunk in response.iter_content(chunk_size):\n",
    "            progress_bar.update(len(chunk))\n",
    "            fh.write(chunk)\n",
    "\n",
    "model = AdaptiveVarNet(num_cascades=args.cascade)\n",
    "AdaptiveVarNet_FOLDER = \"https://dl.fbaipublicfiles.com/active-mri-acquisition/midl_models/adaptive_4x.ckpt/\"\n",
    "MODEL_FNAMES = \"adaptive_4x.ckpt\"\n",
    "\n",
    "url_root = AdaptiveVarNet_FOLDER\n",
    "download_model('https://dl.fbaipublicfiles.com/active-mri-acquisition/midl_models/adaptive_4x.ckpt', MODEL_FNAMES)\n",
    "\n",
    "pretrained = torch.load(MODEL_FNAMES)\n",
    "pretrained_copy = copy.deepcopy(pretrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "65417681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in /home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages (from torchmetrics) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages (from torchmetrics) (1.22.3)\n",
      "Requirement already satisfied: typing_extensions in /home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.9)\n",
      "Installing collected packages: torchmetrics\n",
      "Successfully installed torchmetrics-0.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53838524",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.59 GiB already allocated; 4.19 MiB free; 1.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43madaptive_4x.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:1046\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1044\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1045\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1046\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:1016\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1015\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1016\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:176\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 176\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/serialization.py:158\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m storage_type(obj\u001b[38;5;241m.\u001b[39mnbytes())\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/_utils.py:79\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     new_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/cuda/__init__.py:661\u001b[0m, in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m _lazy_init()\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# We may need to call lazy init again if we are a forked child\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# del _CudaBase.__new__\u001b[39;00m\n\u001b[0;32m--> 661\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_CudaBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 1.59 GiB already allocated; 4.19 MiB free; 1.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load(\"adaptive_4x.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6b4e7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'callbacks', 'optimizer_states', 'lr_schedulers', 'state_dict', 'hparams_name', 'hyper_parameters'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "31989c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.conv.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.conv.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.0.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.0.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.1.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.1.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.2.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.2.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.3.1.weight\n",
      "varnet.sens_net.norm_unet.unet.up_conv.3.1.bias\n",
      "varnet.sens_net.norm_unet.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.sens_net.norm_unet.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.cascades.0.model.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.cascades.0.model.unet.conv.layers.0.weight\n",
      "varnet.cascades.0.model.unet.conv.layers.4.weight\n",
      "varnet.cascades.0.model.unet.up_conv.0.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_conv.0.layers.4.weight\n",
      "varnet.cascades.0.model.unet.up_conv.1.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_conv.1.layers.4.weight\n",
      "varnet.cascades.0.model.unet.up_conv.2.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_conv.2.layers.4.weight\n",
      "varnet.cascades.0.model.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.cascades.0.model.unet.up_conv.3.1.weight\n",
      "varnet.cascades.0.model.unet.up_conv.3.1.bias\n",
      "varnet.cascades.0.model.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.cascades.0.model.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.cascades.1.model.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.cascades.1.model.unet.conv.layers.0.weight\n",
      "varnet.cascades.1.model.unet.conv.layers.4.weight\n",
      "varnet.cascades.1.model.unet.up_conv.0.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_conv.0.layers.4.weight\n",
      "varnet.cascades.1.model.unet.up_conv.1.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_conv.1.layers.4.weight\n",
      "varnet.cascades.1.model.unet.up_conv.2.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_conv.2.layers.4.weight\n",
      "varnet.cascades.1.model.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.cascades.1.model.unet.up_conv.3.1.weight\n",
      "varnet.cascades.1.model.unet.up_conv.3.1.bias\n",
      "varnet.cascades.1.model.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.cascades.1.model.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.cascades.2.model.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.cascades.2.model.unet.conv.layers.0.weight\n",
      "varnet.cascades.2.model.unet.conv.layers.4.weight\n",
      "varnet.cascades.2.model.unet.up_conv.0.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_conv.0.layers.4.weight\n",
      "varnet.cascades.2.model.unet.up_conv.1.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_conv.1.layers.4.weight\n",
      "varnet.cascades.2.model.unet.up_conv.2.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_conv.2.layers.4.weight\n",
      "varnet.cascades.2.model.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.cascades.2.model.unet.up_conv.3.1.weight\n",
      "varnet.cascades.2.model.unet.up_conv.3.1.bias\n",
      "varnet.cascades.2.model.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.cascades.2.model.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.cascades.3.model.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.cascades.3.model.unet.conv.layers.0.weight\n",
      "varnet.cascades.3.model.unet.conv.layers.4.weight\n",
      "varnet.cascades.3.model.unet.up_conv.0.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_conv.0.layers.4.weight\n",
      "varnet.cascades.3.model.unet.up_conv.1.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_conv.1.layers.4.weight\n",
      "varnet.cascades.3.model.unet.up_conv.2.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_conv.2.layers.4.weight\n",
      "varnet.cascades.3.model.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.cascades.3.model.unet.up_conv.3.1.weight\n",
      "varnet.cascades.3.model.unet.up_conv.3.1.bias\n",
      "varnet.cascades.3.model.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.cascades.3.model.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.0.layers.0.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.0.layers.4.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.1.layers.0.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.1.layers.4.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.2.layers.0.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.2.layers.4.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.3.layers.0.weight\n",
      "varnet.cascades.4.model.unet.down_sample_layers.3.layers.4.weight\n",
      "varnet.cascades.4.model.unet.conv.layers.0.weight\n",
      "varnet.cascades.4.model.unet.conv.layers.4.weight\n",
      "varnet.cascades.4.model.unet.up_conv.0.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_conv.0.layers.4.weight\n",
      "varnet.cascades.4.model.unet.up_conv.1.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_conv.1.layers.4.weight\n",
      "varnet.cascades.4.model.unet.up_conv.2.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_conv.2.layers.4.weight\n",
      "varnet.cascades.4.model.unet.up_conv.3.0.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_conv.3.0.layers.4.weight\n",
      "varnet.cascades.4.model.unet.up_conv.3.1.weight\n",
      "varnet.cascades.4.model.unet.up_conv.3.1.bias\n",
      "varnet.cascades.4.model.unet.up_transpose_conv.0.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_transpose_conv.1.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_transpose_conv.2.layers.0.weight\n",
      "varnet.cascades.4.model.unet.up_transpose_conv.3.layers.0.weight\n",
      "varnet.policies.0.sampler.channel_layer.layers.0.weight\n",
      "varnet.policies.0.sampler.channel_layer.layers.0.bias\n",
      "varnet.policies.0.sampler.down_sample_layers.0.layers.0.weight\n",
      "varnet.policies.0.sampler.down_sample_layers.0.layers.0.bias\n",
      "varnet.policies.0.sampler.down_sample_layers.1.layers.0.weight\n",
      "varnet.policies.0.sampler.down_sample_layers.1.layers.0.bias\n",
      "varnet.policies.0.sampler.down_sample_layers.2.layers.0.weight\n",
      "varnet.policies.0.sampler.down_sample_layers.2.layers.0.bias\n",
      "varnet.policies.0.sampler.down_sample_layers.3.layers.0.weight\n",
      "varnet.policies.0.sampler.down_sample_layers.3.layers.0.bias\n",
      "varnet.policies.0.sampler.feature_extractor.0.layers.0.weight\n",
      "varnet.policies.0.sampler.feature_extractor.0.layers.0.bias\n",
      "varnet.policies.0.sampler.feature_extractor.1.layers.0.weight\n",
      "varnet.policies.0.sampler.feature_extractor.1.layers.0.bias\n",
      "varnet.policies.0.sampler.feature_extractor.2.layers.0.weight\n",
      "varnet.policies.0.sampler.feature_extractor.2.layers.0.bias\n",
      "varnet.policies.0.sampler.feature_extractor.3.layers.0.weight\n",
      "varnet.policies.0.sampler.feature_extractor.3.layers.0.bias\n",
      "varnet.policies.0.sampler.feature_extractor.4.layers.0.weight\n",
      "varnet.policies.0.sampler.feature_extractor.4.layers.0.bias\n",
      "varnet.policies.0.sampler.fc_out.0.weight\n",
      "varnet.policies.0.sampler.fc_out.0.bias\n",
      "varnet.policies.0.sampler.fc_out.2.weight\n",
      "varnet.policies.0.sampler.fc_out.2.bias\n",
      "varnet.policies.0.sampler.fc_out.4.weight\n",
      "varnet.policies.0.sampler.fc_out.4.bias\n",
      "loss.w\n"
     ]
    }
   ],
   "source": [
    "for l in ckpt['state_dict']:\n",
    "    print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7da2a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connectome/jubin/.conda/envs/3DCNN/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                not been set for this class (DistributedMetricSum). The property determines if `update` by\n",
      "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                achieved and we recommend setting this to `False`.\n",
      "                We provide an checking function\n",
      "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
      "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                default for now) or if `full_state_update=False` can be used safely.\n",
      "                \n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DistributedMetricSum with abstract method forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madaptive_varnet_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaptiveVarNetModule\n\u001b[0;32m----> 2\u001b[0m pretrained \u001b[38;5;241m=\u001b[39m \u001b[43mAdaptiveVarNetModule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyper_parameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/connectome/jubin/SNU_fastMRI/utils/model/adaptive_varnet_module.py:144\u001b[0m, in \u001b[0;36mAdaptiveVarNetModule.__init__\u001b[0;34m(self, num_cascades, pools, chans, sens_pools, sens_chans, lr, lr_step_size, lr_gamma, weight_decay, budget, cascades_per_policy, loupe_mask, use_softplus, crop_size, num_actions, num_sense_lines, hard_dc, dc_mode, slope, sparse_dc_gradients, straight_through_slope, st_clamp, policy_fc_size, policy_drop_prob, policy_num_fc_layers, policy_activation, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_activation \u001b[38;5;241m=\u001b[39m policy_activation\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# logging functions\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNMSE \u001b[38;5;241m=\u001b[39m \u001b[43mDistributedMetricSum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSSIM \u001b[38;5;241m=\u001b[39m DistributedMetricSum()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPSNR \u001b[38;5;241m=\u001b[39m DistributedMetricSum()\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class DistributedMetricSum with abstract method forward"
     ]
    }
   ],
   "source": [
    "from utils.model.adaptive_varnet_module import AdaptiveVarNetModule\n",
    "pretrained = AdaptiveVarNetModule(**ckpt[\"hyper_parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "488decc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AdaptiveVarNet:\n\tMissing key(s) in state_dict: \"sens_net.norm_unet.unet.down_sample_layers.0.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.0.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.1.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.1.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.2.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.2.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.3.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.3.layers.4.weight\", \"sens_net.norm_unet.unet.conv.layers.0.weight\", \"sens_net.norm_unet.unet.conv.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.0.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.1.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.1.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.2.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.2.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.3.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.3.0.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.3.1.weight\", \"sens_net.norm_unet.unet.up_conv.3.1.bias\", \"sens_net.norm_unet.unet.up_transpose_conv.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.1.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.2.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.3.layers.0.weight\", \"cascades.0.dc_weight\", \"cascades.0.model.unet.down_sample_layers.0.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.0.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.1.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.1.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.2.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.2.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.3.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.3.layers.4.weight\", \"cascades.0.model.unet.conv.layers.0.weight\", \"cascades.0.model.unet.conv.layers.4.weight\", \"cascades.0.model.unet.up_conv.0.layers.0.weight\", \"cascades.0.model.unet.up_conv.0.layers.4.weight\", \"cascades.0.model.unet.up_conv.1.layers.0.weight\", \"cascades.0.model.unet.up_conv.1.layers.4.weight\", \"cascades.0.model.unet.up_conv.2.layers.0.weight\", \"cascades.0.model.unet.up_conv.2.layers.4.weight\", \"cascades.0.model.unet.up_conv.3.0.layers.0.weight\", \"cascades.0.model.unet.up_conv.3.0.layers.4.weight\", \"cascades.0.model.unet.up_conv.3.1.weight\", \"cascades.0.model.unet.up_conv.3.1.bias\", \"cascades.0.model.unet.up_transpose_conv.0.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.1.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.2.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.3.layers.0.weight\", \"policies.0.sampler.channel_layer.layers.0.weight\", \"policies.0.sampler.channel_layer.layers.0.bias\", \"policies.0.sampler.down_sample_layers.0.layers.0.weight\", \"policies.0.sampler.down_sample_layers.0.layers.0.bias\", \"policies.0.sampler.down_sample_layers.1.layers.0.weight\", \"policies.0.sampler.down_sample_layers.1.layers.0.bias\", \"policies.0.sampler.down_sample_layers.2.layers.0.weight\", \"policies.0.sampler.down_sample_layers.2.layers.0.bias\", \"policies.0.sampler.down_sample_layers.3.layers.0.weight\", \"policies.0.sampler.down_sample_layers.3.layers.0.bias\", \"policies.0.sampler.feature_extractor.0.layers.0.weight\", \"policies.0.sampler.feature_extractor.0.layers.0.bias\", \"policies.0.sampler.feature_extractor.1.layers.0.weight\", \"policies.0.sampler.feature_extractor.1.layers.0.bias\", \"policies.0.sampler.feature_extractor.2.layers.0.weight\", \"policies.0.sampler.feature_extractor.2.layers.0.bias\", \"policies.0.sampler.feature_extractor.3.layers.0.weight\", \"policies.0.sampler.feature_extractor.3.layers.0.bias\", \"policies.0.sampler.feature_extractor.4.layers.0.weight\", \"policies.0.sampler.feature_extractor.4.layers.0.bias\", \"policies.0.sampler.fc_out.0.weight\", \"policies.0.sampler.fc_out.0.bias\", \"policies.0.sampler.fc_out.2.weight\", \"policies.0.sampler.fc_out.2.bias\", \"policies.0.sampler.fc_out.4.weight\", \"policies.0.sampler.fc_out.4.bias\". \n\tUnexpected key(s) in state_dict: \"varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.conv.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.conv.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.1.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.2.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.1.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.1.bias\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.0.model.unet.conv.layers.0.weight\", \"varnet.cascades.0.model.unet.conv.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.3.1.weight\", \"varnet.cascades.0.model.unet.up_conv.3.1.bias\", \"varnet.cascades.0.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.1.model.unet.conv.layers.0.weight\", \"varnet.cascades.1.model.unet.conv.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.3.1.weight\", \"varnet.cascades.1.model.unet.up_conv.3.1.bias\", \"varnet.cascades.1.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.2.model.unet.conv.layers.0.weight\", \"varnet.cascades.2.model.unet.conv.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.3.1.weight\", \"varnet.cascades.2.model.unet.up_conv.3.1.bias\", \"varnet.cascades.2.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.3.model.unet.conv.layers.0.weight\", \"varnet.cascades.3.model.unet.conv.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.3.1.weight\", \"varnet.cascades.3.model.unet.up_conv.3.1.bias\", \"varnet.cascades.3.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.4.model.unet.conv.layers.0.weight\", \"varnet.cascades.4.model.unet.conv.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.3.1.weight\", \"varnet.cascades.4.model.unet.up_conv.3.1.bias\", \"varnet.cascades.4.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.policies.0.sampler.channel_layer.layers.0.weight\", \"varnet.policies.0.sampler.channel_layer.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.0.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.0.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.1.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.1.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.2.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.2.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.3.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.3.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.0.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.0.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.1.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.1.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.2.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.2.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.3.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.3.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.4.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.4.layers.0.bias\", \"varnet.policies.0.sampler.fc_out.0.weight\", \"varnet.policies.0.sampler.fc_out.0.bias\", \"varnet.policies.0.sampler.fc_out.2.weight\", \"varnet.policies.0.sampler.fc_out.2.bias\", \"varnet.policies.0.sampler.fc_out.4.weight\", \"varnet.policies.0.sampler.fc_out.4.bias\", \"loss.w\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/3DCNN/lib/python3.9/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AdaptiveVarNet:\n\tMissing key(s) in state_dict: \"sens_net.norm_unet.unet.down_sample_layers.0.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.0.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.1.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.1.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.2.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.2.layers.4.weight\", \"sens_net.norm_unet.unet.down_sample_layers.3.layers.0.weight\", \"sens_net.norm_unet.unet.down_sample_layers.3.layers.4.weight\", \"sens_net.norm_unet.unet.conv.layers.0.weight\", \"sens_net.norm_unet.unet.conv.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.0.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.1.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.1.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.2.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.2.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.3.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_conv.3.0.layers.4.weight\", \"sens_net.norm_unet.unet.up_conv.3.1.weight\", \"sens_net.norm_unet.unet.up_conv.3.1.bias\", \"sens_net.norm_unet.unet.up_transpose_conv.0.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.1.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.2.layers.0.weight\", \"sens_net.norm_unet.unet.up_transpose_conv.3.layers.0.weight\", \"cascades.0.dc_weight\", \"cascades.0.model.unet.down_sample_layers.0.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.0.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.1.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.1.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.2.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.2.layers.4.weight\", \"cascades.0.model.unet.down_sample_layers.3.layers.0.weight\", \"cascades.0.model.unet.down_sample_layers.3.layers.4.weight\", \"cascades.0.model.unet.conv.layers.0.weight\", \"cascades.0.model.unet.conv.layers.4.weight\", \"cascades.0.model.unet.up_conv.0.layers.0.weight\", \"cascades.0.model.unet.up_conv.0.layers.4.weight\", \"cascades.0.model.unet.up_conv.1.layers.0.weight\", \"cascades.0.model.unet.up_conv.1.layers.4.weight\", \"cascades.0.model.unet.up_conv.2.layers.0.weight\", \"cascades.0.model.unet.up_conv.2.layers.4.weight\", \"cascades.0.model.unet.up_conv.3.0.layers.0.weight\", \"cascades.0.model.unet.up_conv.3.0.layers.4.weight\", \"cascades.0.model.unet.up_conv.3.1.weight\", \"cascades.0.model.unet.up_conv.3.1.bias\", \"cascades.0.model.unet.up_transpose_conv.0.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.1.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.2.layers.0.weight\", \"cascades.0.model.unet.up_transpose_conv.3.layers.0.weight\", \"policies.0.sampler.channel_layer.layers.0.weight\", \"policies.0.sampler.channel_layer.layers.0.bias\", \"policies.0.sampler.down_sample_layers.0.layers.0.weight\", \"policies.0.sampler.down_sample_layers.0.layers.0.bias\", \"policies.0.sampler.down_sample_layers.1.layers.0.weight\", \"policies.0.sampler.down_sample_layers.1.layers.0.bias\", \"policies.0.sampler.down_sample_layers.2.layers.0.weight\", \"policies.0.sampler.down_sample_layers.2.layers.0.bias\", \"policies.0.sampler.down_sample_layers.3.layers.0.weight\", \"policies.0.sampler.down_sample_layers.3.layers.0.bias\", \"policies.0.sampler.feature_extractor.0.layers.0.weight\", \"policies.0.sampler.feature_extractor.0.layers.0.bias\", \"policies.0.sampler.feature_extractor.1.layers.0.weight\", \"policies.0.sampler.feature_extractor.1.layers.0.bias\", \"policies.0.sampler.feature_extractor.2.layers.0.weight\", \"policies.0.sampler.feature_extractor.2.layers.0.bias\", \"policies.0.sampler.feature_extractor.3.layers.0.weight\", \"policies.0.sampler.feature_extractor.3.layers.0.bias\", \"policies.0.sampler.feature_extractor.4.layers.0.weight\", \"policies.0.sampler.feature_extractor.4.layers.0.bias\", \"policies.0.sampler.fc_out.0.weight\", \"policies.0.sampler.fc_out.0.bias\", \"policies.0.sampler.fc_out.2.weight\", \"policies.0.sampler.fc_out.2.bias\", \"policies.0.sampler.fc_out.4.weight\", \"policies.0.sampler.fc_out.4.bias\". \n\tUnexpected key(s) in state_dict: \"varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.1.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.2.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.down_sample_layers.3.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.conv.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.conv.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.1.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.2.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.0.layers.4.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.1.weight\", \"varnet.sens_net.norm_unet.unet.up_conv.3.1.bias\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.sens_net.norm_unet.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.0.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.0.model.unet.conv.layers.0.weight\", \"varnet.cascades.0.model.unet.conv.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.0.model.unet.up_conv.3.1.weight\", \"varnet.cascades.0.model.unet.up_conv.3.1.bias\", \"varnet.cascades.0.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.0.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.1.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.1.model.unet.conv.layers.0.weight\", \"varnet.cascades.1.model.unet.conv.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.1.model.unet.up_conv.3.1.weight\", \"varnet.cascades.1.model.unet.up_conv.3.1.bias\", \"varnet.cascades.1.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.1.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.2.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.2.model.unet.conv.layers.0.weight\", \"varnet.cascades.2.model.unet.conv.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.2.model.unet.up_conv.3.1.weight\", \"varnet.cascades.2.model.unet.up_conv.3.1.bias\", \"varnet.cascades.2.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.2.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.3.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.3.model.unet.conv.layers.0.weight\", \"varnet.cascades.3.model.unet.conv.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.3.model.unet.up_conv.3.1.weight\", \"varnet.cascades.3.model.unet.up_conv.3.1.bias\", \"varnet.cascades.3.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.3.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.0.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.0.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.1.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.1.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.2.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.2.layers.4.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.3.layers.0.weight\", \"varnet.cascades.4.model.unet.down_sample_layers.3.layers.4.weight\", \"varnet.cascades.4.model.unet.conv.layers.0.weight\", \"varnet.cascades.4.model.unet.conv.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.0.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.1.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.1.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.2.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.2.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.3.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_conv.3.0.layers.4.weight\", \"varnet.cascades.4.model.unet.up_conv.3.1.weight\", \"varnet.cascades.4.model.unet.up_conv.3.1.bias\", \"varnet.cascades.4.model.unet.up_transpose_conv.0.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.1.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.2.layers.0.weight\", \"varnet.cascades.4.model.unet.up_transpose_conv.3.layers.0.weight\", \"varnet.policies.0.sampler.channel_layer.layers.0.weight\", \"varnet.policies.0.sampler.channel_layer.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.0.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.0.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.1.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.1.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.2.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.2.layers.0.bias\", \"varnet.policies.0.sampler.down_sample_layers.3.layers.0.weight\", \"varnet.policies.0.sampler.down_sample_layers.3.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.0.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.0.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.1.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.1.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.2.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.2.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.3.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.3.layers.0.bias\", \"varnet.policies.0.sampler.feature_extractor.4.layers.0.weight\", \"varnet.policies.0.sampler.feature_extractor.4.layers.0.bias\", \"varnet.policies.0.sampler.fc_out.0.weight\", \"varnet.policies.0.sampler.fc_out.0.bias\", \"varnet.policies.0.sampler.fc_out.2.weight\", \"varnet.policies.0.sampler.fc_out.2.bias\", \"varnet.policies.0.sampler.fc_out.4.weight\", \"varnet.policies.0.sampler.fc_out.4.bias\", \"loss.w\". "
     ]
    }
   ],
   "source": [
    "for layer in pretrained_copy.keys():\n",
    "    if layer.split('.',2)[1].isdigit() and (args.cascade <= int(layer.split('.',2)[1]) <=11):\n",
    "        del pretrained[layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4114ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained_copy.keys():\n",
    "    if layer.split('.',2)[1].isdigit() and (args.cascade <= int(layer.split('.',2)[1]) <=11):\n",
    "        del pretrained[layer]\n",
    "model.load_state_dict(pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c021c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.exp_dir = './result' / Path(args.user) / args.net_name / 'checkpoints'\n",
    "args.val_dir = './result' / Path(args.user) / args.net_name / 'reconstructions_val'\n",
    "args.json_dir = './result' / Path(args.user) / args.net_name / 'jsons'\n",
    "args.main_dir = './result' / Path(args.user) / args.net_name / __file__\n",
    "\n",
    "args.exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "args.val_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"*** Experiment <{args.exp_name}> with model <{args.net_name}> starts ***\")\n",
    "# train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85f22ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_exp_result(save_dir, setting, result, load=''):\n",
    "    for key in setting.copy():\n",
    "        if isinstance(setting[key],Path):\n",
    "            setting[key]=str(setting[key])\n",
    "    for key in setting:\n",
    "        print(key, setting[key])\n",
    "            \n",
    "    exp_name = setting['exp_name']\n",
    "    filename = save_dir / '{}.json'.format(exp_name)\n",
    "    \n",
    "    if load != '':\n",
    "        with open(filename, 'r') as f:\n",
    "            prev_result = json.load(f)\n",
    "        result['train_losses'] = prev_result['train_losses'] + [result['train_losses'][-1]]\n",
    "        result['val_losses'] = prev_result['val_losses'] + [result['val_losses'][-1]]\n",
    "\n",
    "    result.update(setting)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a7a53cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'newUnet_test02.json']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/root/SNU_fastMRI/result/JB/newUnet/jsons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e0a3e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_result = {\"train_losses\": [0.1126745434482247, 0.08656777790693562, 0.07871177827122593, 0.07378690729561649, 0.07049268290504225, 0.06808781471717108, 0.06613185317223144, 0.06456057557926931, 0.06343760484885727, 0.062393130393149955, 0.06151985347685847, 0.06069883476000651, 0.0599703875882443, 0.059316725576006896, 0.05878266602270565, 0.05808181663123748, 0.057610175034286254, 0.05716629564900409, 0.056687517813352864, 0.05626321737838179, 0.055850942007626844, 0.05545629135693862, 0.05508866318417259, 0.05489134027897067, 0.05446442069697546, 0.05420566421097227, 0.05379068007878527, 0.05342896272300844, 0.05311635323300992, 0.05290453547391427, 0.05290313621684736, 0.05290313621684736, 0.052437487583425924, 0.05226639777491098, 0.052061733402951414, 0.051850627857128594, 0.05167068184943321, 0.05147498514148863, 0.05166849390260028, 0.05113865452410173, 0.05105413693563014, 0.05096610848145806, 0.05081353801585682, 0.05095774336370128, 0.050551524848229924, 0.0507162285238297, 0.050575832203203455, 0.050446935845085195, 0.05012047498242761, 0.04999625129655453, 0.04996588680970973, 0.04974741200006755, 0.049968050278533085, 0.04860320224009644, 0.04815446210294754, 0.04800150095725004, 0.047890832833513586, 0.04779776343060203, 0.04771393993891032, 0.047636117846672606, 0.04756143101406761, 0.0474912318165507, 0.04742370611553679, 0.04735887271898805, 0.0472961769303147, 0.04723558514411377, 0.047176470889292846, 0.04709132959560562, 0.04703813155123363, 0.047020926691263294, 0.047008423821832075, 0.04699766940141222, 0.046987873772180826, 0.04697866506200379, 0.0469698192348613, 0.04696129743018449, 0.04695303630939492, 0.046944922609838145, 0.04693701247881294, 0.04692924243944704, 0.04692163199117178, 0.04691414033730456, 0.04690673484049927, 0.04689943693630103, 0.04689220887049168, 0.04688507470619927, 0.04687801522058013, 0.04687102059477167, 0.046864085158726455, 0.04685723850732613, 0.04685045809867487, 0.046823266487387105, 0.046821408647948794, 0.046820097898939095, 0.04681904866357967, 0.04681801560859946, 0.04681709816726897, 0.04681625153238425, 0.04681116010639341, 0.04681095238904665, 0.046810798053121896, 0.046810669024969755, 0.0468105543793769, 0.04681043987207789, 0.046810334907053795, 0.04681023879283542, 0.04681013963615258, 0.04680947140032067, 0.04680946545368558, 0.046809458538993605, 0.0468094507945386, 0.046809445124491186, 0.04680944263520208, 0.046809435582216265, 0.04680942977387501, 0.0468094202316001, 0.04680941663596027, 0.046809411657382054, 0.04680940335975169, 0.04680940405122089, 0.046809393402595253, 0.046809388285723195, 0.046809381509325065, 0.04680937708392221, 0.04680936920117336, 0.046809364499182826, 0.04680935689302166, 0.046809348733685134, 0.046809345691220666, 0.04680933863823486, 0.046809335734064234, 0.04680932923425378, 0.046809321628092614, 0.04680931623463288, 0.046809311670936174, 0.04680930876676555, 0.04680930337330581], \"val_losses\": [0.07701320268789814, 0.06585551620907976, 0.061017491086850666, 0.05846954715903606, 0.05406928298481534, 0.05278321527169123, 0.051271362544789804, 0.0500909573440621, 0.04946734130218826, 0.048750690719284215, 0.0483173459917648, 0.04764226000752933, 0.04702367416726294, 0.04635298759710643, 0.0459594466834779, 0.04548600833345286, 0.04518114024015082, 0.044764938422265366, 0.044362111699176955, 0.04434877992217861, 0.04410303113657932, 0.04373351353738377, 0.04349284792311298, 0.04333410508663855, 0.043107767185128286, 0.04304487510956441, 0.04289999282622632, 0.042629402882912844, 0.042556315126087006, 0.042459442890655404, 0.042235005832180816, 0.042235005832180816, 0.04221791078540079, 0.04216124558345087, 0.04207138159420431, 0.04197790294883636, 0.04197377291905489, 0.04179987619726175, 0.041921154199984025, 0.04184543466425959, 0.04193928585824301, 0.041715172190197734, 0.04171321947668402, 0.04158482951727707, 0.0418606002927049, 0.0416446015800981, 0.04149748928447408, 0.041770541248421854, 0.041778352928221026, 0.04200054122886407, 0.04196268720814386, 0.04257735269278013, 0.04172716872220217, 0.03973842909735647, 0.03965190089933748, 0.03961216693755756, 0.03959025181301756, 0.03957630606006218, 0.039568538750724136, 0.03956297619832298, 0.03955894439978465, 0.03955689453882156, 0.0395551410592463, 0.03955509466330273, 0.03955510925184342, 0.03955561622201829, 0.039555090148746676, 0.039388655159594964, 0.03937280789612834, 0.03936580836583552, 0.03936187736752048, 0.039359117303746936, 0.03935706307151727, 0.03935548986890743, 0.039354236039784635, 0.039353068318736334, 0.039351875729441134, 0.03935082971798344, 0.03934979994007595, 0.0393489188481803, 0.03934824217847903, 0.03934753002820568, 0.03934685509100581, 0.03934613287460026, 0.039345416581820516, 0.03934478669329759, 0.03934422496842617, 0.03934367351497681, 0.03934307040528762, 0.03934260542203555, 0.03934202428155458, 0.039257600411805496, 0.03925669721769365, 0.03925626580566328, 0.03925496995290252, 0.03925509665857566, 0.039255431210932785, 0.03925460798325049, 0.03925408992729895, 0.03925368091413328, 0.03925338135551504, 0.03925323773044325, 0.039252990334401154, 0.03925297073089265, 0.039252921026669226, 0.03925283456611146, 0.03925280343893089, 0.039252789415031936, 0.03925277656089387, 0.039252763882364164, 0.03925275111368668, 0.039252737404680584, 0.03925272545838465, 0.039252713214662595, 0.039252699307674485, 0.03925269073650476, 0.03925267902714311, 0.03925266759613818, 0.03925265647905403, 0.03925264662687492, 0.039252635527930284, 0.03925262517115569, 0.03925261507435313, 0.03925260493523185, 0.039252595036429735, 0.039252585007752104, 0.03925257529036179, 0.03925256544851727, 0.03925255586906909, 0.03925254635221903, 0.039252537006831935, 0.03925252821987165, 0.03925251992033688, 0.03925251046251668, 0.03925250160188047, 0.039252492898538396, 0.03925248634948134], \"GPU_NUM\": 0, \"batch_size\": 12, \"num_epochs\": 60, \"lr\": 1e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d475aa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prev_result['val_losses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa4a425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(path, 'w') as f:\n",
    "        json.dump(prev_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1e8ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "path = Path('/root/SNU_fastMRI/result/JB/newUnet/jsons/newUnet_test02.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a114dddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU_NUM 0\n",
      "batch_size 256\n",
      "num_epochs 3\n",
      "lr 0.001\n",
      "report_interval 2\n",
      "net_name test_Unet\n",
      "optim Adam\n",
      "scheduler Plateau\n",
      "data_path_train /root/input/train/image\n",
      "data_path_val /root/input/val/image\n",
      "in_chans 1\n",
      "out_chans 1\n",
      "input_key image_input\n",
      "target_key image_label\n",
      "max_key max\n",
      "load \n",
      "exp_name test\n",
      "{'train_losses': [0.1126745434482247, 0.08656777790693562, 0.07871177827122593, 0.07378690729561649, 0.07049268290504225, 0.06808781471717108, 0.06613185317223144, 0.06456057557926931, 0.06343760484885727, 0.062393130393149955, 0.06151985347685847, 0.06069883476000651, 0.0599703875882443, 0.059316725576006896, 0.05878266602270565, 0.05808181663123748, 0.057610175034286254, 0.05716629564900409, 0.056687517813352864, 0.05626321737838179, 0.055850942007626844, 0.05545629135693862, 0.05508866318417259, 0.05489134027897067, 0.05446442069697546, 0.05420566421097227, 0.05379068007878527, 0.05342896272300844, 0.05311635323300992, 0.05290453547391427, 0.05290313621684736, 0.05290313621684736, 0.052437487583425924, 0.05226639777491098, 0.052061733402951414, 0.051850627857128594, 0.05167068184943321, 0.05147498514148863, 0.05166849390260028, 0.05113865452410173, 0.05105413693563014, 0.05096610848145806, 0.05081353801585682, 0.05095774336370128, 0.050551524848229924, 0.0507162285238297, 0.050575832203203455, 0.050446935845085195, 0.05012047498242761, 0.04999625129655453, 0.04996588680970973, 0.04974741200006755, 0.049968050278533085, 0.04860320224009644, 0.04815446210294754, 0.04800150095725004, 0.047890832833513586, 0.04779776343060203, 0.04771393993891032, 0.047636117846672606, 0.04756143101406761, 0.0474912318165507, 0.04742370611553679, 0.04735887271898805, 0.0472961769303147, 0.04723558514411377, 0.047176470889292846, 0.04709132959560562, 0.04703813155123363, 0.047020926691263294, 0.047008423821832075, 0.04699766940141222, 0.046987873772180826, 0.04697866506200379, 0.0469698192348613, 0.04696129743018449, 0.04695303630939492, 0.046944922609838145, 0.04693701247881294, 0.04692924243944704, 0.04692163199117178, 0.04691414033730456, 0.04690673484049927, 0.04689943693630103, 0.04689220887049168, 0.04688507470619927, 0.04687801522058013, 0.04687102059477167, 0.046864085158726455, 0.04685723850732613, 0.04685045809867487, 0.046823266487387105, 0.046821408647948794, 0.046820097898939095, 0.04681904866357967, 0.04681801560859946, 0.04681709816726897, 0.04681625153238425, 0.04681116010639341, 0.04681095238904665, 0.046810798053121896, 0.046810669024969755, 0.0468105543793769, 0.04681043987207789, 0.046810334907053795, 0.04681023879283542, 0.04681013963615258, 0.04680947140032067, 0.04680946545368558, 0.046809458538993605, 0.0468094507945386, 0.046809445124491186, 0.04680944263520208, 0.046809435582216265, 0.04680942977387501, 0.0468094202316001, 0.04680941663596027, 0.046809411657382054, 0.04680940335975169, 0.04680940405122089, 0.046809393402595253, 0.046809388285723195, 0.046809381509325065, 0.04680937708392221, 0.04680936920117336, 0.046809364499182826, 0.04680935689302166, 0.046809348733685134, 0.046809345691220666, 0.04680933863823486, 0.046809335734064234, 0.04680932923425378, 0.046809321628092614, 0.04680931623463288, 0.046809311670936174, 0.04680930876676555, 0.04680930337330581], 'val_losses': [0.07701320268789814, 0.06585551620907976, 0.061017491086850666, 0.05846954715903606, 0.05406928298481534, 0.05278321527169123, 0.051271362544789804, 0.0500909573440621, 0.04946734130218826, 0.048750690719284215, 0.0483173459917648, 0.04764226000752933, 0.04702367416726294, 0.04635298759710643, 0.0459594466834779, 0.04548600833345286, 0.04518114024015082, 0.044764938422265366, 0.044362111699176955, 0.04434877992217861, 0.04410303113657932, 0.04373351353738377, 0.04349284792311298, 0.04333410508663855, 0.043107767185128286, 0.04304487510956441, 0.04289999282622632, 0.042629402882912844, 0.042556315126087006, 0.042459442890655404, 0.042235005832180816, 0.042235005832180816, 0.04221791078540079, 0.04216124558345087, 0.04207138159420431, 0.04197790294883636, 0.04197377291905489, 0.04179987619726175, 0.041921154199984025, 0.04184543466425959, 0.04193928585824301, 0.041715172190197734, 0.04171321947668402, 0.04158482951727707, 0.0418606002927049, 0.0416446015800981, 0.04149748928447408, 0.041770541248421854, 0.041778352928221026, 0.04200054122886407, 0.04196268720814386, 0.04257735269278013, 0.04172716872220217, 0.03973842909735647, 0.03965190089933748, 0.03961216693755756, 0.03959025181301756, 0.03957630606006218, 0.039568538750724136, 0.03956297619832298, 0.03955894439978465, 0.03955689453882156, 0.0395551410592463, 0.03955509466330273, 0.03955510925184342, 0.03955561622201829, 0.039555090148746676, 0.039388655159594964, 0.03937280789612834, 0.03936580836583552, 0.03936187736752048, 0.039359117303746936, 0.03935706307151727, 0.03935548986890743, 0.039354236039784635, 0.039353068318736334, 0.039351875729441134, 0.03935082971798344, 0.03934979994007595, 0.0393489188481803, 0.03934824217847903, 0.03934753002820568, 0.03934685509100581, 0.03934613287460026, 0.039345416581820516, 0.03934478669329759, 0.03934422496842617, 0.03934367351497681, 0.03934307040528762, 0.03934260542203555, 0.03934202428155458, 0.039257600411805496, 0.03925669721769365, 0.03925626580566328, 0.03925496995290252, 0.03925509665857566, 0.039255431210932785, 0.03925460798325049, 0.03925408992729895, 0.03925368091413328, 0.03925338135551504, 0.03925323773044325, 0.039252990334401154, 0.03925297073089265, 0.039252921026669226, 0.03925283456611146, 0.03925280343893089, 0.039252789415031936, 0.03925277656089387, 0.039252763882364164, 0.03925275111368668, 0.039252737404680584, 0.03925272545838465, 0.039252713214662595, 0.039252699307674485, 0.03925269073650476, 0.03925267902714311, 0.03925266759613818, 0.03925265647905403, 0.03925264662687492, 0.039252635527930284, 0.03925262517115569, 0.03925261507435313, 0.03925260493523185, 0.039252595036429735, 0.039252585007752104, 0.03925257529036179, 0.03925256544851727, 0.03925255586906909, 0.03925254635221903, 0.039252537006831935, 0.03925252821987165, 0.03925251992033688, 0.03925251046251668, 0.03925250160188047, 0.039252492898538396, 0.03925248634948134], 'GPU_NUM': 0, 'batch_size': 256, 'num_epochs': 3, 'lr': 0.001, 'report_interval': 2, 'net_name': 'test_Unet', 'optim': 'Adam', 'scheduler': 'Plateau', 'data_path_train': '/root/input/train/image', 'data_path_val': '/root/input/val/image', 'in_chans': 1, 'out_chans': 1, 'input_key': 'image_input', 'target_key': 'image_label', 'max_key': 'max', 'load': '', 'exp_name': 'test'}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "save_exp_result(path, deepcopy(vars(args)), deepcopy(prev_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e468c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69af89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Full on Python 3.6 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
